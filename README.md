1. With the Assignment, given were the two set of files result_x and result_y(classes) and a score of 0.01 or less to achieve.
2. First the creation of Byte-bigrams feature was most important.
3. For that a system with 16 gb ram and core i7 processor was acquired, with all the anaconda packages. As the number of files was large
so multiprocessing had to be done but the python packages for multiprocessing were not able to hit the maximum capacity of ram or
cpu cores on a windows system, so a naive but working method of running multiple jupyter notbooks on different sets of file was done.
This took around 3 hrs and resulted in a 1.6 GB csv file with 66050 features after merger.
4. With the help of 'dchad' presentation, conversion of byte/asm files to image files were done and another 2000 features were created.
5. The byte-bigram.csv file was too big to load in memory at once because of the sparsity.
6. All the possible tries(size reduction with PCA,SVD,featureimportance, permutation_importance,or directly split convert and then merge)
to change the byte-bigram files(1.6gb sparse file) in a sparse coo_matrix failed as even 25gb ram(colab) was taking a hit on memory and
the acquired pc was no more available to create a sparse matrix all over again.
7. Intense hyperparameter tunning(shown here is the last part of hyperparameter tunnning, methods used were 2 step grid search/
zooming in the search space on singluar features/ optuna or baysian search methods.) was done to strike the perfect balance between
variance and bais using only the provided files and image features to achieve the score of 0.0089 on test files
